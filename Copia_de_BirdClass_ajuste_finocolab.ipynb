{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIGhu4eoggfn"
      },
      "source": [
        "# Clasificación Aviar en Ibagué mediante Deep Learning: Enfoque en Precisión y Eficiencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujmQTVkRggfs"
      },
      "source": [
        "El proceso de clasificación aviar en Ibagué, mediante técnicas de Deep Learning, reviste una importancia fundamental en la conservación y el estudio de la biodiversidad de la región. Ibagué, ubicada en una zona geográfica rica en diversidad de aves, se enfrenta a desafíos significativos en la identificación y monitoreo de estas especies. La implementación de un sistema multiclase de identificación aviar no solo facilita la tarea de catalogación para investigadores y conservacionistas, sino que también proporciona una herramienta eficaz para la evaluación del estado de las poblaciones aviares y la detección temprana de posibles amenazas, como la pérdida de hábitat o el cambio climático. Con un enfoque en la precisión y eficiencia del proceso de clasificación, este sistema puede contribuir de manera significativa a la gestión sostenible de los recursos naturales y la protección del medio ambiente en la región de Ibagué."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wed2fUzEggfu"
      },
      "source": [
        "## Librerias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvJjDuJpggfv"
      },
      "source": [
        "A continuación se relacionana algunas de las bibliotecas y módulos de Python que se utilizan comúnmente en el desarrollo de aplicaciones de aprendizaje automático,  procesamiento de imágenes y que se utilizaran en este proyecto:\n",
        "\n",
        "1. **Keras**: Es una biblioteca de redes neuronales de código abierto escrita en Python que facilita la creación y entrenamiento de modelos de aprendizaje profundo. Proporciona una interfaz simple y consistente para construir y entrenar modelos de redes neuronales.\n",
        "\n",
        "2. **NumPy**: Es una biblioteca fundamental para computación numérica en Python. Se utiliza para realizar operaciones matemáticas en matrices y matrices multidimensionales, lo que es esencial para el procesamiento de datos en el aprendizaje automático.\n",
        "\n",
        "3. **OpenCV (cv2)**: OpenCV (Open Source Computer Vision Library) es una biblioteca de código abierto que se utiliza para el procesamiento de imágenes y visión por computadora. Proporciona una amplia gama de funciones y algoritmos para tareas como manipulación de imágenes, detección de características, reconocimiento de objetos, seguimiento de objetos, entre otros.\n",
        "\n",
        "4. **Matplotlib**: Es una biblioteca de trazado en 2D de Python que produce figuras de calidad de publicación en una variedad de formatos y entornos. Se utiliza para visualizar datos y resultados, incluidas imágenes y gráficos.\n",
        "\n",
        "5. **TensorFlow**: Es una biblioteca de código abierto desarrollada por Google para el aprendizaje automático y la inteligencia artificial. TensorFlow proporciona un ecosistema completo para construir y entrenar modelos de aprendizaje profundo, incluidas API de alto nivel como Keras.\n",
        "\n",
        "6. **Scikit-learn**: Es una biblioteca de aprendizaje automático de código abierto que proporciona herramientas simples y eficientes para el análisis predictivo y la minería de datos. Incluye una variedad de algoritmos de aprendizaje supervisado y no supervisado, así como herramientas para preprocesamiento de datos, evaluación de modelos y selección de características.\n",
        "\n",
        "En resumen, estas bibliotecas son componentes clave en el desarrollo de aplicaciones de aprendizaje automático y procesamiento de imágenes, proporcionando herramientas y funciones para construir, entrenar, evaluar y visualizar modelos de manera eficiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y_7hrN3ggfx"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eCJwy2FM6rO",
        "outputId": "6a12fda9-79d6-4b34-b673-dc06a07fb188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcQlRUJXggf0"
      },
      "source": [
        "# Configuración de parámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUY6-RXfggf1"
      },
      "source": [
        "Es crucial establecer los parámetros específicos, como el tamaño de las imágenes de entrada (width_shape y height_shape), el número de clases a predecir (num_classes), así como los hiperparámetros de entrenamiento, como el número de épocas (epochs) y el tamaño del lote (batch_size), antes de iniciar el proceso de entrenamiento de un modelo de aprendizaje profundo para la clasificación de imágenes. Estos parámetros tienen un impacto significativo en la calidad y eficiencia del modelo resultante. Por ejemplo, el tamaño de las imágenes de entrada afecta la capacidad del modelo para capturar detalles importantes de las imágenes, mientras que el número de clases determina la complejidad de la tarea de clasificación. Además, la configuración adecuada de los hiperparámetros de entrenamiento, como el número de épocas y el tamaño del lote, es esencial para garantizar un entrenamiento efectivo y eficiente del modelo, evitando el sobreajuste o el subajuste. Por lo tanto, la correcta especificación de estos parámetros es fundamental para lograr resultados precisos y confiables en el proceso de clasificación de imágenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqvM_WLzggf1"
      },
      "outputs": [],
      "source": [
        "width_shape = 224\n",
        "height_shape = 224\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SrwuG4mggf2"
      },
      "source": [
        "Este código define algunas variables que son comunes al entrenar modelos de aprendizaje profundo para clasificación de imágenes:\n",
        "\n",
        "- `width_shape` y `height_shape`: Estas variables representan el ancho y la altura de las imágenes de entrada al modelo. En este caso, las imágenes se están redimensionando a una forma cuadrada de 224x224 píxeles. Es común redimensionar las imágenes de entrada a un tamaño específico antes de alimentarlas al modelo.\n",
        "\n",
        "- `num_classes`: Esta variable indica el número de clases en el problema de clasificación. En este caso, el modelo se entrenará para clasificar imágenes en 10 clases diferentes.\n",
        "\n",
        "- `epochs`: Este parámetro especifica el número de épocas o iteraciones completas sobre el conjunto de datos durante el entrenamiento del modelo. Cada época implica pasar por todo el conjunto de datos una vez hacia adelante y hacia atrás a través de la red neuronal.\n",
        "\n",
        "- `batch_size`: Este parámetro indica el tamaño del lote de datos que se utilizará en cada iteración de entrenamiento. Durante el entrenamiento, los datos se dividen en lotes y se procesan en paralelo, lo que puede acelerar el proceso de entrenamiento y hacerlo más eficiente en términos de uso de memoria. Un valor típico para `batch_size` es 32, lo que significa que se procesarán 32 imágenes a la vez durante cada iteración de entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi-sSgegggf2"
      },
      "source": [
        "# Path de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf6Dt89Tggf3"
      },
      "source": [
        "Estos son los directorios donde se almacenan los datos de entrenamiento y validación respectivamente para el modelo. Es una práctica común dividir el conjunto de datos en un conjunto de entrenamiento y un conjunto de validación para evaluar el rendimiento del modelo durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWsoQGtrggf4"
      },
      "outputs": [],
      "source": [
        "train_data_dir = '/content/drive/MyDrive/Colab Notebooks/TFMaves/dataset/train'\n",
        "validation_data_dir = '/content/drive/MyDrive/Colab Notebooks/TFMaves/dataset/valid'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTFSJcK4ggf4"
      },
      "source": [
        "\n",
        "- `train_data_dir`: Este directorio contiene las imágenes de entrenamiento. Por lo general, se espera que contenga subdirectorios separados para cada clase (cada tipo de ave) de imagen, donde las imágenes de cada clase se agrupan juntas.\n",
        "\n",
        "- `validation_data_dir`: Este directorio contiene las imágenes de validación, que se utilizan para evaluar el rendimiento del modelo en un conjunto de datos independiente durante el entrenamiento. Al igual que con el directorio de entrenamiento, se espera que este directorio contenga subdirectorios separados para cada clase de imagen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4szPvddYggf5"
      },
      "source": [
        "La estructura de directorios esperada por los generadores de datos de Keras o TensorFlow para tareas de clasificación de imágenes suele ser la siguiente:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── train/\n",
        "│   ├── class1/\n",
        "│   │   ├── image1.jpg\n",
        "│   │   ├── image2.jpg\n",
        "│   │   └── ...\n",
        "│   ├── class2/\n",
        "│   │   ├── image1.jpg\n",
        "│   │   ├── image2.jpg\n",
        "│   │   └── ...\n",
        "│   └── ...\n",
        "└── valid/\n",
        "    ├── class1/\n",
        "    │   ├── image1.jpg\n",
        "    │   ├── image2.jpg\n",
        "    │   └── ...\n",
        "    ├── class2/\n",
        "    │   ├── image1.jpg\n",
        "    │   ├── image2.jpg\n",
        "    │   └── ...\n",
        "    └── ...\n",
        "```\n",
        "\n",
        "En esta estructura:\n",
        "\n",
        "- El directorio `train` contiene subdirectorios separados para cada clase de imagen.\n",
        "- Cada subdirectorio de clase (`class1`, `class2`, etc.) contiene las imágenes correspondientes a esa clase.\n",
        "- El directorio `valid` tiene una estructura similar al directorio de entrenamiento y se utiliza para almacenar las imágenes de validación.\n",
        "\n",
        "Es importante seguir esta estructura de directorios para que los generadores de datos de Keras o TensorFlow puedan encontrar y cargar correctamente las imágenes durante el entrenamiento y la validación del modelo. Si los datos no están organizados de esta manera, los generadores de datos pueden tener dificultades para encontrar las imágenes y el proceso de entrenamiento puede fallar o producir resultados incorrectos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPD1GeAVggf6"
      },
      "source": [
        "# Generador de imágenes para las imagenes de `train`  y `valid`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb_wGSEvggf7"
      },
      "source": [
        "Los generadores de imágenes son fundamentales para cargar y procesar los datos de entrenamiento y validación de manera eficiente durante el entrenamiento del modelo de clasificación de imágenes. Además, aplican aumentos de datos en tiempo real durante el entrenamiento, lo que ayuda a mejorar la capacidad del modelo para generalizar a nuevos datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIAOTPkAggf7",
        "outputId": "9b8c396c-40f9-4b79-9c73-9e0d9b0f3f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10000 images belonging to 10 classes.\n",
            "Found 960 images belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "# Definir el generador de imágenes para el conjunto de entrenamiento con aumentos de datos\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,               # Rango de grados para rotación aleatoria\n",
        "    zoom_range=0.2,                  # Rango de zoom aleatorio\n",
        "    width_shift_range=0.1,           # Rango de desplazamiento horizontal aleatorio\n",
        "    height_shift_range=0.1,          # Rango de desplazamiento vertical aleatorio\n",
        "    horizontal_flip=True,            # Volteo horizontal aleatorio\n",
        "    vertical_flip=False,             # No se aplica volteo vertical\n",
        "    preprocessing_function=preprocess_input)  # Función de preprocesamiento\n",
        "\n",
        "# Definir el generador de imágenes para el conjunto de validación con los mismos aumentos de datos\n",
        "valid_datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    preprocessing_function=preprocess_input)\n",
        "\n",
        "# Crear un generador de lotes de imágenes para el conjunto de entrenamiento\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,                  # Directorio que contiene las imágenes de entrenamiento\n",
        "    target_size=(width_shape, height_shape),  # Tamaño al que se redimensionarán las imágenes\n",
        "    batch_size=batch_size,           # Tamaño del lote\n",
        "    class_mode='categorical')        # Modo de clasificación para imágenes categóricas\n",
        "\n",
        "# Crear un generador de lotes de imágenes para el conjunto de validación\n",
        "validation_generator = valid_datagen.flow_from_directory(\n",
        "    validation_data_dir,             # Directorio que contiene las imágenes de validación\n",
        "    target_size=(width_shape, height_shape),  # Tamaño al que se redimensionarán las imágenes\n",
        "    batch_size=batch_size,           # Tamaño del lote\n",
        "    class_mode='categorical')        # Modo de clasificación para imágenes categóricas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N29yMdyiggf9"
      },
      "source": [
        "# Entrenamiento de modelo VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bujoli6ggf-"
      },
      "source": [
        "A continuación, se configura y entrena un modelo de red neuronal convolucional (CNN) utilizando la arquitectura VGG16 preentrenada para la clasificación de imágenes. Primero, se establecen variables para el número de muestras de entrenamiento y validación. Luego, se define la estructura de entrada de la red neuronal y se carga el modelo VGG16 preentrenado con sus pesos ajustados a partir del conjunto de datos ImageNet. Se añade una capa densa adicional para adaptar la salida a la cantidad de clases en el problema de clasificación y se congela el resto de las capas para evitar que se modifiquen durante el entrenamiento. Posteriormente, el modelo se compila con una función de pérdida y un optimizador específicos, y se muestra un resumen de la arquitectura del modelo. Finalmente, se lleva a cabo el entrenamiento del modelo utilizando generadores de datos para el conjunto de entrenamiento y validación, con la configuración de épocas y pasos definida previamente. Durante el entrenamiento, se actualizan únicamente los pesos de la capa densa añadida, mientras que los pesos de las capas preentrenadas permanecen constantes debido a la congelación previa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp6TOG2dggf-",
        "outputId": "b4c1f011-04d2-413f-c04d-419806d8a535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 3s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " output (Dense)              (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 134301514 (512.32 MB)\n",
            "Trainable params: 40970 (160.04 KB)\n",
            "Non-trainable params: 134260544 (512.16 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "312/312 [==============================] - 2729s 9s/step - loss: 0.9461 - accuracy: 0.6987 - val_loss: 0.5958 - val_accuracy: 0.8042\n",
            "Epoch 2/50\n",
            "312/312 [==============================] - 260s 832ms/step - loss: 0.4036 - accuracy: 0.8723 - val_loss: 0.4625 - val_accuracy: 0.8531\n",
            "Epoch 3/50\n",
            "312/312 [==============================] - 261s 835ms/step - loss: 0.3252 - accuracy: 0.8952 - val_loss: 0.3949 - val_accuracy: 0.8781\n",
            "Epoch 4/50\n",
            "312/312 [==============================] - 261s 835ms/step - loss: 0.2792 - accuracy: 0.9092 - val_loss: 0.3613 - val_accuracy: 0.8844\n",
            "Epoch 5/50\n",
            "312/312 [==============================] - 259s 831ms/step - loss: 0.2515 - accuracy: 0.9193 - val_loss: 0.3542 - val_accuracy: 0.8896\n",
            "Epoch 6/50\n",
            "312/312 [==============================] - 258s 826ms/step - loss: 0.2416 - accuracy: 0.9210 - val_loss: 0.3464 - val_accuracy: 0.8990\n",
            "Epoch 7/50\n",
            "312/312 [==============================] - 257s 824ms/step - loss: 0.2226 - accuracy: 0.9270 - val_loss: 0.3355 - val_accuracy: 0.9052\n",
            "Epoch 8/50\n",
            "289/312 [==========================>...] - ETA: 17s - loss: 0.2113 - accuracy: 0.9299"
          ]
        }
      ],
      "source": [
        "# Definir el número de muestras de entrenamiento y validación\n",
        "nb_train_samples = 10000\n",
        "nb_validation_samples = 960\n",
        "\n",
        "# Definir la entrada de la red neuronal con el tamaño de las imágenes\n",
        "image_input = Input(shape=(width_shape, height_shape, 3))\n",
        "\n",
        "# Cargar el modelo VGG16 preentrenado con pesos ajustados desde ImageNet\n",
        "model = VGG16(input_tensor=image_input, include_top=True, weights='imagenet')\n",
        "\n",
        "# Obtener la salida de la penúltima capa densa del modelo VGG16 (fc2)\n",
        "last_layer = model.get_layer('fc2').output\n",
        "\n",
        "# Añadir una nueva capa densa al final del modelo para la clasificación multiclase\n",
        "out = Dense(num_classes, activation='softmax', name='output')(last_layer)\n",
        "\n",
        "# Crear un nuevo modelo personalizado que toma la entrada de la imagen y produce la salida clasificada\n",
        "custom_vgg_model = Model(image_input, out)\n",
        "\n",
        "# Congelar todas las capas del modelo, excepto la capa densa añadida\n",
        "for layer in custom_vgg_model.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compilar el modelo con una función de pérdida, optimizador y métricas especificadas\n",
        "custom_vgg_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Mostrar un resumen del modelo que incluye la arquitectura y el número de parámetros\n",
        "custom_vgg_model.summary()\n",
        "\n",
        "# Entrenar el modelo utilizando generadores de datos para el conjunto de entrenamiento y validación\n",
        "model_history = custom_vgg_model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=nb_train_samples//batch_size,  # Número de pasos por época de entrenamiento\n",
        "    validation_steps=nb_validation_samples//batch_size)  # Número de pasos por época de validación\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JNCEr67ggf_"
      },
      "source": [
        "Este código realiza las siguientes operaciones en general:\n",
        "\n",
        "1. **Definición de variables:** Se definen dos variables `nb_train_samples` y `nb_validation_samples` para indicar el número de muestras de entrenamiento y validación respectivamente. Estas variables se utilizan más adelante para configurar el entrenamiento del modelo.\n",
        "\n",
        "2. **Definición de la entrada de la red neuronal:** Se define la entrada de la red neuronal utilizando la clase `Input` de Keras. La forma de la entrada se especifica como `(width_shape, height_shape, 3)`, lo que indica el tamaño de las imágenes de entrada y el número de canales de color (en este caso, 3 para imágenes RGB).\n",
        "\n",
        "3. **Carga del modelo preentrenado VGG16:** Se carga el modelo preentrenado VGG16 utilizando la función `VGG16` de Keras. Se especifica el tensor de entrada de la red neuronal (`input_tensor=image_input`), se incluye la capa densa al final del modelo (`include_top=True`) y se cargan los pesos preentrenados en el conjunto de datos de ImageNet (`weights='imagenet'`).\n",
        "\n",
        "4. **Creación de un nuevo modelo personalizado:** Se crea un nuevo modelo personalizado que incluye todas las capas de la VGG16 hasta la última capa densa (`fc2`). Se agrega una capa densa adicional al final del modelo con un número de unidades igual al número de clases en el problema (`num_classes`). Esta capa tiene una función de activación softmax, lo que la convierte en una clasificación de salida.\n",
        "\n",
        "5. **Congelación de capas:** Se congela todas las capas del modelo preentrenado VGG16, excepto la última capa densa añadida. Esto se hace para que durante el entrenamiento solo se actualicen los pesos de la nueva capa densa.\n",
        "\n",
        "6. **Compilación del modelo:** Se compila el modelo personalizado utilizando la función de pérdida `categorical_crossentropy`, el optimizador `adadelta` y se incluye la métrica de precisión (`accuracy`).\n",
        "\n",
        "7. **Resumen del modelo:** Se muestra un resumen del modelo, que incluye todas las capas y el número total de parámetros entrenables y no entrenables.\n",
        "\n",
        "8. **Entrenamiento del modelo:** Se entrena el modelo utilizando los generadores de datos `train_generator` y `validation_generator` que se definieron anteriormente. Se especifica el número de épocas (`epochs`) y el número de pasos por época (`steps_per_epoch`) y de validación (`validation_steps`). Durante el entrenamiento, se actualizan los pesos de la capa densa añadida mientras que los pesos de las capas de VGG16 se mantienen fijos debido a la congelación realizada anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKZrBmk5gggA"
      },
      "source": [
        "# Grabar modelo en disco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voYkjja4gggA"
      },
      "source": [
        "Guardar el modelo tiene varias ventajas. En primer lugar, permite la reutilización futura, ya que puedes cargarlo y utilizarlo nuevamente sin necesidad de volver a entrenarlo desde cero, lo que resulta útil tanto para hacer predicciones en nuevos datos como para continuar el entrenamiento en una fecha posterior. Además, facilita la distribución y compartición del modelo con otros investigadores, colegas o clientes que puedan necesitar utilizarlo en sus propios proyectos. Por último, al guardar el modelo junto con su configuración y pesos entrenados, se asegura la reproducibilidad de los resultados, ya que otros investigadores pueden cargar el modelo exacto y obtener los mismos resultados que tú, lo que es fundamental para la validación y la comparación de resultados en investigación científica y desarrollo de modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whzki-WdgggB"
      },
      "outputs": [],
      "source": [
        "custom_vgg_model.save(\"models/model_VGG16_v2_os.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AiIWlUjgggB"
      },
      "source": [
        "Este código utilizará un bucle while para verificar si el archivo con el nombre base del modelo ya existe. Si existe, agregará un número al final del nombre del archivo y verificará nuevamente. Esto continuará hasta que se encuentre un nombre de archivo único que no exista en el directorio. Una vez que se encuentra un nombre único, el modelo se guarda con ese nombre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbTJMCQcgggB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Nombre base del modelo\n",
        "model_name = \"model_VGG16_v\"\n",
        "\n",
        "# Extensión del archivo\n",
        "file_extension = \".keras\"\n",
        "\n",
        "# Directorio donde se guardarán los modelos\n",
        "model_directory = \"models/\"\n",
        "\n",
        "# Inicializar contador\n",
        "counter = 1\n",
        "\n",
        "# Generar el nombre completo del archivo\n",
        "file_name = model_name + file_extension\n",
        "\n",
        "ruta=model_directory + file_name\n",
        "print(ruta)\n",
        "# Verificar si el modelo ya está guardado\n",
        "while os.path.exists(model_directory + file_name):\n",
        "\n",
        "    # Si el archivo existe, agregar un número al final del nombre del modelo\n",
        "    file_name = f\"{model_name}{counter}{file_extension}\"\n",
        "    counter += 1\n",
        "\n",
        "# Guardar el modelo con el nombre único en el directorio correcto\n",
        "custom_vgg_model.save(model_directory + file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z0fkxqJgggC"
      },
      "source": [
        "# Gráficas de entrenamiento y validación (accuracy - loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF_5BTYigggD"
      },
      "outputs": [],
      "source": [
        "def plotTraining(hist, epochs, typeData):\n",
        "\n",
        "    if typeData==\"loss\":\n",
        "        plt.figure(1,figsize=(10,5))\n",
        "        yc=hist.history['loss']\n",
        "        xc=range(epochs)\n",
        "        plt.ylabel('Loss', fontsize=24)\n",
        "        plt.plot(xc,yc,'-r',label='Loss Training')\n",
        "    if typeData==\"accuracy\":\n",
        "        plt.figure(2,figsize=(10,5))\n",
        "        yc=hist.history['accuracy']\n",
        "        for i in range(0, len(yc)):\n",
        "            yc[i]=100*yc[i]\n",
        "        xc=range(epochs)\n",
        "        plt.ylabel('Accuracy (%)', fontsize=24)\n",
        "        plt.plot(xc,yc,'-r',label='Accuracy Training')\n",
        "    if typeData==\"val_loss\":\n",
        "        plt.figure(1,figsize=(10,5))\n",
        "        yc=hist.history['val_loss']\n",
        "        xc=range(epochs)\n",
        "        plt.ylabel('Loss', fontsize=24)\n",
        "        plt.plot(xc,yc,'--b',label='Loss Validate')\n",
        "    if typeData==\"val_accuracy\":\n",
        "        plt.figure(2,figsize=(10,5))\n",
        "        yc=hist.history['val_accuracy']\n",
        "        for i in range(0, len(yc)):\n",
        "            yc[i]=100*yc[i]\n",
        "        xc=range(epochs)\n",
        "        plt.ylabel('Accuracy (%)', fontsize=24)\n",
        "        plt.plot(xc,yc,'--b',label='Training Validate')\n",
        "\n",
        "\n",
        "    plt.rc('xtick',labelsize=24)\n",
        "    plt.rc('ytick',labelsize=24)\n",
        "    plt.rc('legend', fontsize=18)\n",
        "    plt.legend()\n",
        "    plt.xlabel('Number of Epochs',fontsize=24)\n",
        "    plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK2zg6S4gggE"
      },
      "outputs": [],
      "source": [
        "plotTraining(model_history,epochs,\"loss\")\n",
        "plotTraining(model_history,epochs,\"accuracy\")\n",
        "plotTraining(model_history,epochs,\"val_loss\")\n",
        "plotTraining(model_history,epochs,\"val_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH71JXBCgggE"
      },
      "source": [
        "# Predicción usando el modelo entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t_UbnEKgggF"
      },
      "outputs": [],
      "source": [
        "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
        "from keras.models import load_model\n",
        "\n",
        "names = ['AFRICAN FIREFINCH','ALBATROSS','ALEXANDRINE PARAKEET','AMERICAN AVOCET','AMERICAN BITTERN',\n",
        "         'AMERICAN COOT','AMERICAN GOLDFINCH','AMERICAN KESTREL','AMERICAN PIPIT','AMERICAN REDSTART']\n",
        "\n",
        "modelt = load_model(\"models/model_VGG16_v3.keras\")\n",
        "#modelt = custom_vgg_model\n",
        "\n",
        "imaget_path = \"ImagenPrueba.jpg\"\n",
        "imaget=cv2.resize(cv2.imread(imaget_path), (width_shape, height_shape), interpolation = cv2.INTER_AREA)\n",
        "xt = np.asarray(imaget)\n",
        "xt=preprocess_input(xt)\n",
        "xt = np.expand_dims(xt,axis=0)\n",
        "preds = modelt.predict(xt)\n",
        "\n",
        "print(names[np.argmax(preds)])\n",
        "plt.imshow(cv2.cvtColor(np.asarray(imaget),cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn0QVpvDgggG"
      },
      "source": [
        "# Matriz de confusión y métricas de desempeño"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVBKpw51gggG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, precision_score, recall_score, accuracy_score, roc_auc_score\n",
        "from sklearn import metrics\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "names = ['AFRICAN FIREFINCH','ALBATROSS','ALEXANDRINE PARAKEET','AMERICAN AVOCET','AMERICAN BITTERN',\n",
        "         'AMERICAN COOT','AMERICAN GOLDFINCH','AMERICAN KESTREL','AMERICAN PIPIT','AMERICAN REDSTART']\n",
        "\n",
        "\n",
        "test_data_dir = 'C:/Users/DIAZOVIEDO/Documents/GitHub/TFMaves/dataset/test'\n",
        "\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(width_shape, height_shape),\n",
        "    batch_size = batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False)\n",
        "\n",
        "custom_Model= load_model(\"models/model_VGG16_v1.keras\")\n",
        "\n",
        "predictions = custom_Model.predict(test_generator)\n",
        "\n",
        "\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "y_real = test_generator.classes\n",
        "\n",
        "\n",
        "matc=confusion_matrix(y_real, y_pred)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=matc, figsize=(9,9), class_names = names, show_normed=False)\n",
        "plt.tight_layout()\n",
        "\n",
        "print(metrics.classification_report(y_real,y_pred, digits = 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPxXY5t4gggH"
      },
      "source": [
        "# Transfer Learning modelo VGG16 - fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irzn1_CRgggI"
      },
      "outputs": [],
      "source": [
        "# Importar las funciones necesarias de Keras\n",
        "from keras.layers import Input\n",
        "from keras.applications import VGG16\n",
        "\n",
        "# Definir la forma de entrada para las imágenes (ancho, alto, canales RGB)\n",
        "image_input = Input(shape=(width_shape, height_shape, 3))\n",
        "\n",
        "# Crear el modelo VGG16 utilizando la entrada de imagen definida\n",
        "# include_top=True significa que se incluirán todas las capas densas en la parte superior del modelo\n",
        "# weights='imagenet' significa que se utilizarán los pesos pre-entrenados en ImageNet\n",
        "model2 = VGG16(input_tensor=image_input, include_top=True, weights='imagenet')\n",
        "\n",
        "# Mostrar un resumen de la arquitectura del modelo\n",
        "model2.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTbft2ULgggI"
      },
      "outputs": [],
      "source": [
        "last_layer = model2.get_layer('block5_pool').output\n",
        "x= Flatten(name='flatten')(last_layer)\n",
        "x = Dense(128, activation='relu', name='fc1')(x)\n",
        "x = Dense(128, activation='relu', name='fc2')(x)\n",
        "out = Dense(num_classes, activation='softmax', name='output')(x)\n",
        "custom_model = Model(image_input, out)\n",
        "custom_model.summary()\n",
        "\n",
        "# freeze all the layers except the dense layers\n",
        "for layer in custom_model.layers[:-3]:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "custom_model.summary()\n",
        "\n",
        "custom_model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-6nRtY7gggJ"
      },
      "outputs": [],
      "source": [
        "# Definir el número de muestras de entrenamiento y validación\n",
        "nb_train_samples = 1490\n",
        "nb_validation_samples = 50\n",
        "\n",
        "model_history = custom_model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    steps_per_epoch=nb_train_samples//batch_size,\n",
        "    validation_steps=nb_validation_samples//batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oCPOnldgggJ"
      },
      "outputs": [],
      "source": [
        "custom_model.save(\"models/model_VGG16_50.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL8X5FaJgggK"
      },
      "outputs": [],
      "source": [
        "def plotTraining(hist, epochs, typeData):\n",
        "\n",
        "    if typeData==\"loss\":\n",
        "        plt.figure(1,figsize=(10,5))\n",
        "        yc=hist.history['loss']\n",
        "        xc=range(epochs)\n",
        "        plt.ylabel('Loss', fontsize=24)\n",
        "        plt.plot(xc,yc,'-r',label='Loss Training')\n",
        "    if typeData==\"accuracy\":\n",
        "        plt.figure(2,figsize=(10,5))\n",
        "        yc=hist.history['accuracy']\n",
        "        for i in range(0, len(yc)):\n",
        "            yc[i]=100*yc[i]\n",
        "        xc=range(epochs)\n",
        "        plt.ylabel('Accuracy (%)', fontsize=24)\n",
        "        plt.plot(xc,yc,'-r',label='Accuracy Training')\n",
        "    if typeData==\"val_loss\":\n",
        "        plt.figure(1,figsize=(10,5))\n",
        "        yc=hist.history['val_loss']\n",
        "        xc=range(epochs)\n",
        "        plt.ylabel('Loss', fontsize=24)\n",
        "        plt.plot(xc,yc,'--b',label='Loss Validate')\n",
        "    if typeData==\"val_accuracy\":\n",
        "        plt.figure(2,figsize=(10,5))\n",
        "        yc=hist.history['val_accuracy']\n",
        "        for i in range(0, len(yc)):\n",
        "            yc[i]=100*yc[i]\n",
        "        xc=range(epochs)\n",
        "        plt.ylabel('Accuracy (%)', fontsize=24)\n",
        "        plt.plot(xc,yc,'--b',label='Training Validate')\n",
        "\n",
        "\n",
        "    plt.rc('xtick',labelsize=24)\n",
        "    plt.rc('ytick',labelsize=24)\n",
        "    plt.rc('legend', fontsize=18)\n",
        "    plt.legend()\n",
        "    plt.xlabel('Number of Epochs',fontsize=24)\n",
        "    plt.grid(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_qjuGLogggM"
      },
      "outputs": [],
      "source": [
        "plotTraining(model_history,epochs,\"loss\")\n",
        "plotTraining(model_history,epochs,\"accuracy\")\n",
        "plotTraining(model_history,epochs,\"val_loss\")\n",
        "plotTraining(model_history,epochs,\"val_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8OV6Yn2gggN"
      },
      "source": [
        "La validación oscila tanto porque podría haber un problema de sobreajuste (overfitting) o la necesidad de ajustar los hiperparámetros. El modelo parece estar aprendiendo bien durante el entrenamiento, como se muestra en la línea roja continua “Accuracy Training”, pero tiene un rendimiento inconsistente en los datos de validación, indicado por la línea azul punteada “Training Validate”.\n",
        "\n",
        "Para abordar esto:\n",
        "\n",
        "Regularización: Introduce técnicas de regularización como dropout o weight decay para reducir el sobreajuste.\n",
        "Ajuste de Hiperparámetros: Experimenta con diferentes valores para hiperparámetros como tasa de aprendizaje, número de capas ocultas, etc.\n",
        "Más Datos: Si es posible, aumenta el tamaño del conjunto de datos de entrenamiento.\n",
        "Early Stopping: Detén el entrenamiento cuando la precisión de validación deja de mejorar.\n",
        "El gráfico muestra que el modelo necesita más ajustes para generalizar mejor a datos no vistos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE5YXX_fgggO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, precision_score, recall_score, accuracy_score, roc_auc_score\n",
        "from sklearn import metrics\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "names = ['AFRICAN FIREFINCH','ALBATROSS','ALEXANDRINE PARAKEET','AMERICAN AVOCET','AMERICAN BITTERN',\n",
        "         'AMERICAN COOT','AMERICAN GOLDFINCH','AMERICAN KESTREL','AMERICAN PIPIT','AMERICAN REDSTART']\n",
        "\n",
        "\n",
        "test_data_dir = 'C:/Users/DIAZOVIEDO/Documents/GitHub/TFMaves/dataset/test'\n",
        "\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(width_shape, height_shape),\n",
        "    batch_size = batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False)\n",
        "\n",
        "custom_Model50= load_model(\"models/model_VGG16_50.keras\")\n",
        "\n",
        "predictions = custom_Model50.predict(test_generator)\n",
        "\n",
        "\n",
        "y_pred = np.argmax(predictions, axis=1)\n",
        "y_real = test_generator.classes\n",
        "\n",
        "\n",
        "matc=confusion_matrix(y_real, y_pred)\n",
        "\n",
        "plot_confusion_matrix(conf_mat=matc, figsize=(9,9), class_names = names, show_normed=False)\n",
        "plt.tight_layout()\n",
        "\n",
        "print(metrics.classification_report(y_real,y_pred, digits = 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcCNmUOtgggO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}